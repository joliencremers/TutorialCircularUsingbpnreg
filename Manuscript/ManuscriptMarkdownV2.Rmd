---
title: "One direction? A tutorial for circular data in cognitive psychology."

author: |
  | Jolien Cremers^[Corresponding author: j.cremers@uu.nl] $^1$, Irene Klugkist $^{1,2}$
  | $^1$Department of Methodology and Statistics, Utrecht University
  | $^2$Research methodology, Measurement and Data Analysis, Behavioural Sciences, University of Twente
  
abstract: |


keywords: 


output:
  pdf_document:
    keep_tex: yes
    fig_caption: true
    latex_engine: pdflatex
    number_sections: true
  html_document: default

fontsize: 11pt
geometry: margin=1in
documentclass: article
csl: apa.csl
pandoc_args: --natbib
bibliography: CircularData.bib

header-includes:
 # - \setlength{\parindent}{4em}
 # - \setlength{\parskip}{0em}
 # - \usepackage{endfloat}    
 # - \usepackage{setspace}\doublespacing
 # - \usepackage{lineno}
 # - \linenumbers

---

```{r inlude = FALSE, echo = FALSE}

knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```


```{r, echo = FALSE}

library(circular)
library(bpnreg)

```

\section{Introduction}\label{Introduction}

Circular data arises in almost all fields of research, from ecology where data
on the direction movement of animals is investigated [@rivest2015general] to the
medical sciences where protein structure [@mardia2007protein] or neuronal
activity [@rutishauser2010human] is investigated using periodic and thus
circular measurements. The most direct examples of circular data within the
social sciences arise in cognitive and experimental psychology. For example, in
experiments on cognitive maps the human sense of direction is investigated
through asking participants in a study to point north [@brunye2015map] or to
walk to a certain object [@warren2017wormholes]. The closer the participants'
pointing or walking direction was to the actual north or target object, the
better their sense of direction. Other examples include the visual perception of
space [@matsushima2014independence], visual working memory
[@heyes2016longitudinal] and sensorimotor synchronization in music making
[@kirschner2009joint].

However, despite the fact that circular data is being collected in different 
areas of cognitive and experimental psychology, the knowledge of this type of 
data is not well-spread. Circular data is fundamentally different from linear 
data due to its periodic nature. On the circle, measurements at 0$^\circ$ and 
360$^\circ$ represent the same direction whereas on a linear scale they would be
located at opposite ends of a scale. For this reason circular data require 
specific analysis methods. Some less technical textbooks on analysis methods for
circular data have been written [@fisher1995statistical; @Batschelet1981;
@pewsey2013circular]. However these works are not part of the 'standard' texts 
on statistical analysis in psychology or the social sciences in general nor are
they very well known amongst social scientific researchers.

Therefore, this paper aims at giving a tutorial in working with and analysing 
circular data to researchers in congintive psychology and the social sciencesin
general.  The main goal of this tutorial is to explain how to inspect and
analyse your data when the outcome variable is circular. We will discuss data
inspection, model fit, estimation and hypothesis testing in general linear
models (GLM) and mixed-effects models.  In this tutorial we decide to mainly
focus on one particular approach to the analysis of circular data, the embedding
approach. We do so for the flexibility of this approach and the resulting
variety in types of models that have already been outlined in the literature on
circular data for this approach. Note that for an optimal understanding of the
paper, the reader should ideally have some knowledge on \verb|R| and on multiple
regression and mixed-effects models in the linear setting. The reader does not
need to be familiar with circular data.

The structure of the tutorial is such that the reader is guided by two examples 
throughout the paper. One is an example for an ANOVA model and the other for a 
mixed-effects model. First however, we give a short introduction to circular 
data in general in Section \ref{circdat}. Next, in Section \ref{DataInspection},
we introduce the ANOVA example after which descriptive methods for circular data
are explained through a section on data inspection for this example. After that 
we will continue with an analysis of the example datasets. First we analyse the 
ANOVA dataset using a method for circular GLM and give interpretation guidelines
for this model in Section \ref{RegModel}. Next, in Section \ref{MEModel} we will
introduce and analyse the mixed-effects example data. Again, we analyse this
data and include guidelines for interpretation. The analyses of both datasets,
the ANOVA and mixed-effects dataset, are performed using the package
\verb|bpnreg|. For both models we use, the GLM and the mixed-effects model for a
circular outcome we write a short technical section (sections
\ref{EmbeddingApproach} and \ref{embeddingME}) in which the mathematical details
of the respective models are given. Lastly, a summary of the paper and
additional references to literature on other models for circular data are given
in Section \ref{conclusion}.





\section{Circular data}\label{circdat}

In the introduction we have briefly mentioned that circular data is data that 
has a periodic nature. The most intuitive form of circular data comes in the 
form of directions on a compass. For example, a participant in an experiment 
could be instructed to move or point to a certain target. We can then measure 
the direction, North, South, East or West on a scale from 0 to 360 degrees. A 
plot of such measurements for several participants is shown in Figure 
\ref{plotdir}. In this plot we can easily see the periodicity of the data, 
0$^\circ$ represents the same datapoint as 360$^\circ$. Furthermore, we can see
what happens if we would treat this data in the 'usual' linear way. Participants
that moved North-East have a score of 45$^\circ$ and participants that moves
South-East have a score of 315$^\circ$. On the circle we can see that this is
only 90$^\circ$ apart, while on a linear scale it is much further apart at 
315$^\circ$ - 45$^\circ$ = 270$^\circ$. More importantly however, there is a
difference between the circular and linear means for this data. In Figure
\ref{plotdir} we see that the circular mean direction is 0$^\circ$. The linear
mean however is 180$^\circ$ and is opposite to the actual mean direction of the
data. Clearly, a linear treatment of the data in Figure \ref{plotdir} can lead
to incorrect conclusions.

```{r plotdir, cache = TRUE, warning = FALSE, dev = "tikz",fig.ext="svg", echo = FALSE, results = FALSE}

require(tikzDevice)

tikz("plotdir.tex", standAlone =TRUE, height = 4, width = 8, pointsize = 12, engine = "pdftex")

dat <- rvonmises(n=50, mu=circular(0), kappa=4)

par(mfrow = c(1,2))

plot(dat, axes = FALSE, ticks = FALSE)
axis.circular(at=circular(seq(0, 2*pi-pi/2, pi/2)), 
              labels=c("0", "90", "180", "270"), cex = 0.8)
plot(dat, axes = FALSE, ticks = FALSE)
axis.circular(at=circular(seq(0, 2*pi-pi/2, pi/2)), 
              labels=c("E", "N", "W", "S"), cex = 0.8)

dev.off() 

tools::texi2dvi("plotdir.tex",pdf=TRUE)

```

\begin{figure}
        \centering

\includegraphics[width=\textwidth]{plotdir.pdf}

         \caption{Data from participants in an experiment that were instructed to move East. The plot on the left shows the data on a 0$^\circ$ - 360$^\circ$ scale. The plot on the right shows the data on the compass.}
		\label{plotdir}
\end{figure}

Clock times are another type of circular data. We might for instance be 
interested at what time of day a certain event takes place, e.g. the time of day
at which positive affect is highest. Figure \ref{plothour} shows data the time 
of day at which positive affect is highest for two groups of participants, e.g. 
two groups of psychiatrical patients who are being treated for depression at 
different clinics. From the plots we clearly see that the peak of positive 
affect for the two groups is at roughly the same time of day, one slightly 
before 12 pm and one slighly after. However, if we were to analyze this data 
using standard statistics for linear data and we would compare the means of the 
two groups, 11 pm and 1 am we would reach a completely different conclusion. The
two means are namely at the two opposite ends of a linear scale from 00.00 am to
12.00 pm, and we would counclude that the time of day at which positive affect
is highest is different for the two groups.

```{r plothour, cache = TRUE, warning = FALSE, dev = "tikz",fig.ext="svg", echo = FALSE, results = FALSE}

require(tikzDevice)

tikz("plothour.tex", standAlone =TRUE, height = 4, width = 8, pointsize = 12, engine = "pdftex")

g1 <- rvonmises(n=50, mu=circular(0.2), kappa=6)
g2 <- rvonmises(n=50, mu=circular(6), kappa=6)

par(mfrow = c(1,2))

plot(g1, template = "clock24", main = "group 1")
plot(g2, template = "clock24", main = "group 2")

dev.off() 

tools::texi2dvi("plothour.tex",pdf=TRUE)

```

\begin{figure}
        \centering

\includegraphics[width=\textwidth]{plothour.pdf}

         \caption{Data for the hour at which positive affect is highest for two groups of
psychiatrical patients who are being treated for depression at different
clinics.}
		\label{plothour}
\end{figure}

The two examples of circular data that we have just given illustrate why it is 
important to treat circular data differently from linear data. This goes both 
for describing your data, e.g. computing circular means as well as analyzing 
them, e.g. testing whether the circular means of two groups differ. In the next 
section we will introduce an example dataset on which we will show several ways
to inspect and compute descriptive measures for circular data.





\section{Inspecting your data}\label{DataInspection}

Before any statistical analysis it is wise to inspect your data. In this section
we will outline a basic way to do so using the \verb|R| packages \verb|bpnreg|
and \verb|circular| [@circularpackage]. We will illustrate this for an example
dataset, the motor resonance data, introduced below.


\subsection{The motor resonance data}\label{regressionexample}

In this section we introduce data from an article by @puglisi2017role on human 
motor resonance. From now on we will call this data the motor resonance data. 
Motor resonance is a response in the brain in the primary motor cortex and 
spinal circuits that is caused by observation of others' actions. In their 
research @puglisi2017role conduct an experiment in which 'observers'  are asked 
to either look at the movement of a hand of a 'mover' or at another other object
in order to evaluate the role of attention in motor resonant response. The 
experiment has three conditions: the 'explicit observation' condition (n = 14), 
where observers are explicitly instructed to observe the hand, the 
'semi-implicit observation' condition (n = 14) where the observers have to 
perform a task that requires the implicit observation of the hand of the mover 
and the 'implicit observation' condition (n = 14), where observers have to 
perform a task that is independent of the observation of the hand of the mover. 
The idea of motor resonance is that the 'observer' starts moving his or her hand
in the same manner as the 'mover' because he or she is implicitly or explicitly 
observing the hand of the 'mover'. This is the resonant response. This resonant 
response is hypothesized to be strongest and more synchronized with the hand
movement of the mover in the explicit condition and smallest in the implicit
condition. In each condition the hand movements of the observers were measured
and the phase difference between movement of the the observers' hand and the
hand of the mover was calculated. This phase difference is a measurement of the
strength of the resonant response and a circular variable. It can thus be
described and analyzed using circular statistics. In addition to the phase
difference the average amplitute of the hand movement of the observer was 
computed. Note that in the original article there was also a baseline condition 
(n=14) without a mover. Instead, observers had to look at an inanimate object 
that moves in an identical manner to the hand of the mover in other conditions. 
The baseline condition is however not included in the example data since no 
resonant response was observerd in the observers' hand according to the original
research.

The motor resonance data can be found in the package \verb|bpnreg| as the
dataframe \verb|Motor|. \verb|Motor| is a dataframe with 42 rows and 7
variables. The variable \verb|Cond| indicates the condition (explicit,
semi-implicit and implicit) a participant was placed in, the \verb|AvAmp|
variable contains the average amplitude, and that the \verb|PhaseDiff| and
\verb|Phaserad| variables contain the measured phase difference between
'observer' and 'mover' in degrees and radians respectively.


\subsection{Plots for circular data}\label{Plots}

```{r, echo = FALSE}
exp <- subset(Motor, Cond == "exp")$PhaseDiff
sem.imp <-subset(Motor,  Cond == "semi.imp")$PhaseDiff
imp <- subset(Motor, Cond == "imp")$PhaseDiff

exp <- as.circular(exp, units = "degrees")
sem.imp <- as.circular(sem.imp, units = "degrees")
imp <- as.circular(imp, units = "degrees")
```

```{r, eval = FALSE, echo = FALSE}
plot(exp, units = "degrees", main = "Explicit observation")
plot(sem.imp, units = "degrees", main = "Semi-implicit observation")
plot(imp, units = "degrees", main = "Implicit observation")

```

```{r plotMotor, cache = TRUE, warning = FALSE, dev = "tikz",fig.ext="svg", echo = FALSE, results = FALSE}

require(tikzDevice)

tikz("plot_motorresonance.tex", standAlone =TRUE, height = 4, width = 8, pointsize = 12, engine = "pdftex") 

par(mfrow = c(1,3))
par(mar=c(1,1,1,1)) 

plot(exp, units = "degrees", main = "Explicit observation", cex = 1.5)
plot(sem.imp, units = "degrees", main = "Semi-implicit observation", cex = 1.5)
plot(imp, units = "degrees", main = "Implicit observation", cex = 1.5)

dev.off() 

tools::texi2dvi("plot_motorresonance.tex",pdf=TRUE)
```

```{r,echo = FALSE, results = FALSE}
par(mfrow = c(1,1))
par(mar=c(5.1, 4.1, 4.1, 2.1))
```


The main question of interest for the motor resonance data is whether the phase 
difference between the three experimental conditions differs. To be more precise
whether there is a smaller phase difference in the explicit condition than in 
the other two. Differences that are observed in the phase difference are 
interpreted as differences in the strength of the resonant response 
[@puglisi2017role]. A smaller phase difference indicates a stronger an more
synchronized resonant response.  A first step to investigating the question of
interest is plotting the phase differences of the three conditions. We can do so
using the package \verb|circular|.

\begin{figure}
        \centering

\includegraphics[width=\textwidth]{plot_motorresonance.pdf}

         \caption{Plots of the phase differences for each condition of the motor resonance data.}
		\label{motorplot}
\end{figure}

Figure \ref{motorplot} shows the plots of the phase differences of each
condition. We see that the phase differences in the explicit condition are much
less spread out on the circle than the phase differences in the other two
conditions. Also the average phase differences seem to differ between the
conditions. In the next section we show measures for the mean and variance of a
sample of circular data.


\subsection{Circular mean, resultant length and variance}\label{MeanVariance}

```{r descriptives, echo = FALSE, results = FALSE}

#circular mean
mean_circ(exp, units = "degrees")
mean_circ(imp, units = "degrees")
mean_circ(sem.imp, units = "degrees")

#mean resultant length
rho_circ(exp, units = "degrees")
rho_circ(imp, units = "degrees")
rho_circ(sem.imp, units = "degrees")

#circular sd
sd_circ(exp, units = "degrees")
sd_circ(imp, units = "degrees")
sd_circ(sem.imp, units = "degrees")

```

\begin{table}
\centering
\caption{Descriptives for the motor resonance data with mean direction ($\bar{\theta}$), mean resultant length ($\bar{R}$), circular variance ($V_m$) and circular standard deviation ($v$) of the phase difference for each condition.} 
\begin{tabular}{lrrrr}
  \hline\noalign{\smallskip}
Phase difference & $\bar{\theta}$ & $\bar{R}$ & $V_m$ & $v$ \\ \hline\noalign{\smallskip}
explicit         & 49.55$^{\circ}$ & 0.77 & 0.23 & 41.39$^{\circ}$\\ 
semi.implicit    & 18.45$^{\circ}$ & 0.54 & 0.46 & 63.82$^{\circ}$\\
implicit         & 31.94$^{\circ}$ & 0.56 & 0.44 & 61.72$^{\circ}$\\
   \hline
\end{tabular}
\label{TableDescriptives}
\end{table}

Table \ref{TableDescriptives} shows descriptives for the motor resonance data.
For each group, the table contains a circular mean and a mean resultant length
of the phase difference. The circular population mean, $\mu$, indicates the
average direction of a certain variable in the population. The population mean
resultant length, $\rho$, is a statistic between 0 and 1 that gives us
information on the spread of a circular variable in the population. It is
interpreted as a precision measure where 0 means that the spread is large and 1
means that all data are concentrated at a single value. Sample statistics for
these values are $\bar{\theta}$ for the mean and $\bar{R}$ for the mean
resultant length. @fisher1995statistical explains how to compute these two
statistics. 

Graphically we can illustrate their computation as shown in Figure 
\ref{meanrbarplot}. On the left side of this figure we see two sets of circular 
data. We represent represent a circular datapoint as a vector composed of the
cosine and sine of the datapoint instead of one value a measure in degrees (or
radians). E.g. for a score of 90$^\circ$ we have the following vector 
$(\cos(90^\circ), \sin(90^\circ))$. The solid vectors in Figure
\ref{meanbarplot} each represent one circular datapoint. To compute the mean
directions and resultant lengths for the datasets on the left we place the
vectors head to toe, as in the right side of Figure \ref{meanrbarplot}. We then
connect the toe of the first vector to the head of the last vector. This results
in the dotted vectors on the right side of Figure \ref{meanrbarplot}. The
direction of the dotted vector is the mean direction, $\bar{\theta}$ of the
vectors from which it was created. The length of the dotted vector is the
resultant length. The mean resultant length, $\bar{R}$ is the length of this
vector divided by the amount of vectors from which it was created. In Figure
\ref{meanrbarplot} we see that the data in the bottom left figure are much more
concentrated on the circle than the data in the upper left figure. This
translates to the resultant length in the bottom right being larger than the
resultant length (length of the dotted vector) in the upper left.

```{r meanrbar, cache = TRUE, dev = "tikz",fig.ext="svg", results = FALSE, echo = FALSE}

tikz("meanrbar.tex", standAlone =TRUE, height = 4, width = 4, pointsize = 12, engine = "pdftex")

par(mfrow = c(2,2),
    oma=c(0,0,0,0),
    mar=c(0,0,0,0),
    las=1, mgp=c(4,1,0),
    xaxs="r", yaxs="r")

plot(0, 0,  asp = 1, bty='n', axes = FALSE, xlab = "", ylab = "",  xlim = c(-1, 3), ylim = c(-1, 2))
draw.circle(0, 0, 1)
arrows(c(0, 0, 0), c(0, 0, 0), cos(c(1, 0.5, 5)), sin(c(1, 0.5, 5)), length = 0.05)

plot(0, 0,  asp = 1, bty='n', axes = FALSE, xlab = "", ylab = "",  xaxs="r", yaxs="r", xlim = c(-1, 3), ylim = c(-1, 2))
draw.circle(0, 0, 1)
arrows(c(0, cos(1), cos(1) + cos(0.5)),
       c(0, sin(1), sin(1) + sin(0.5)),
       c(cos(1), cos(1) + cos(0.5), cos(1) + cos(0.5) + cos(5)),
       c(sin(1), sin(1) + sin(0.5), sin(1) + sin(0.5) + sin(5)),
       length = 0.05)
arrows(0, 0, cos(1) + cos(0.5) + cos(5), sin(1) + sin(0.5) + sin(5), lty = 2, length = 0.05)
#arrows(0, 0, (cos(1) + cos(0.5) + cos(5))/3, (sin(1) + sin(0.5) + sin(5))/3, col = "deepskyblue", length = 0.05)

plot(0, 0,  asp = 1, bty='n', axes = FALSE, xlab = "", ylab = "",  xaxs="r", yaxs="r", xlim = c(-1, 3), ylim = c(-1, 2))
draw.circle(0, 0, 1)
arrows(c(0, 0, 0), c(0, 0, 0), cos(c(0.3, 0.4, 0.5)), sin(c(0.3, 0.4, 0.5)), length = 0.05)

plot(0, 0,  asp = 1, bty='n', axes = FALSE, xlab = "", ylab = "",  xaxs="r", yaxs="r", xlim = c(-1, 3), ylim = c(-1, 2))
draw.circle(0, 0, 1)
arrows(c(0, cos(0.3), cos(0.3) + cos(0.4)),
       c(0, sin(0.3), sin(0.3) + sin(0.4)),
       c(cos(0.3), cos(0.3) + cos(0.4), cos(0.3) + cos(0.4) + cos(0.5)),
       c(sin(0.3), sin(0.3) + sin(0.4), sin(0.3) + sin(0.4) + sin(0.5)),
       length = 0.05)
arrows(0, 0, cos(0.3) + cos(0.4) + cos(0.5), sin(0.3) + sin(0.4) + sin(0.5), lty = 2, length = 0.05)
#arrows(0, 0, (cos(0.3) + cos(0.4) + cos(0.5))/3, (sin(0.3) + sin(0.4) + sin(0.5))/3, col = "deepskyblue", length = 0.05)

dev.off()

tools::texi2dvi("meanrbar.tex",pdf=TRUE)
```

\begin{figure}
        \centering

\includegraphics[width=\textwidth]{meanrbar.pdf}

         \caption{The computation of a circular mean and (mean) resultant length (right) for two sets of circular data (left). The solid lines are vectors representing the circular datapoints. The direction of the dotted vector is the mean direction and the length of the dotted vector is the resultant length.}
		\label{meanrbarplot}
\end{figure}

In Table \ref{TableDescriptives} we see that in the motor resonance data the
circular mean of the phase difference for the explicit observation condition is
highest with 49.55$^{\circ}$. The mean phase differences for the semi-implicit
and implicit observation conditions are lower at 18.45$^{\circ}$ and
31.94$^{\circ}$. Moreover, the mean resultant lengths of the three groups
differ. The phase differences of the individuals in the explicit observation
condition are most concentrated with $\bar{R} = 0.77$. The phase differences
differ more between the individuals in the semi-implicit and implicit
observation conditions where the spread is larger at $\bar{R}$'s of 0.54 and
0.56 respectively.

Table \ref{TableDescriptives} also shows a circular variance and standard
deviation. The sample value, $V_m$ for the circular variance is defined as
$1-\bar{R}$. Its interpretation is exactly opposite to the interpretation of the
mean resultant length. A variance of one means that a variable has a very large
spread and a variance of 0 means that all data are concentrated at one point.
Note that unlike a linear variance the circular variance is bounded between 0
and 1. A sample circular standard deviation, $v$, can also be computed
[@fisher1995statistical]. 

We have seen that both the average phase difference and variances of the phase
difference seem to be different for the three conditions in the motor resonance
data. To test whether these differences in circular means also exist in the
population, we can use a a projected normal regression model. In the next
section we will introduce this model and fit it to the motor resonance data.





\section{A general linear model with a circular outcome}\label{RegModel}

In this section we will introduce a projected normal circular regression model. 
This model falls within the embedding approach to circular data. Note that 
because it is a regression model we can also fit AN(C)OVA type models with it, 
we can thus refer to it as a circular GLM. As noted in the introduction, we
choose to focus on a model from the embedding approach since this approach is
quite flexible in the sense that it is relatively easy to translate models that
exist for linear data to the circular case. We will first outline the
theoretical background to the projected normal circular regression model and the
embedding approach after which we will continue to fit an ANOVA to the motor
resonance data. At the end of this section we will shortly consider two
different methods for circular ANOVA.


\subsection{The embedding approach to circular data}\label{EmbeddingApproach}

In the previous section, at the computation of the circular mean, we have seen
that a circular variable $\theta$, e.g. the phase difference in the motor
resonance data, can be expressed as a unit vector $\boldsymbol{u}$ composed of
the sine and the cosine of an angle $\boldsymbol{u} = (\cos\theta, \sin\theta)$.
If we translate this to bivariate real space the cosine is the x-component and
the sine is the y-component of an angle. In the embedding approach we assume
that the circular variable $\boldsymbol{u}$ origins from the following relation:

\begin{equation}\label{embedding}
 \boldsymbol{u} = \boldsymbol{y} / r 
\end{equation}

We assume that $\boldsymbol{u}$ can be multiplied by a positive linear variable
$r$, $0 < r \leq \infty$, to obtain a bivariate normal variable
$\boldsymbol{y}$. Note that in a real dataset with sample size $n$ we have a set
of vectors $\boldsymbol{u}_{i}$, one for each person $i = 1, \dots, n$. This
also means that there is an underlying vector $\boldsymbol{y}_{i}$ and value
$r_{i}$ for each person in our dataset. Figure \ref{projectingplot} depicts this
relation between $\boldsymbol{u}_{i}$ and $\boldsymbol{y}_{i}$.

```{r plotprojecting, cache = TRUE, warning = FALSE, dev = "tikz",fig.ext="svg", echo = FALSE, results = FALSE}
library(MASS)
library(plotrix)

set.seed(10200)

Y <- mvrnorm(10, c(2,2), (diag(2)))

theta <- atan2(Y[,2], Y[,1])

tikz("plotprojecting.tex", standAlone =TRUE, height = 3, width = 3, pointsize = 12, engine = "pdftex")

par(oma =c(0,0,0,0), mar = c(0,0,0,0))

plot(x = Y[,1], y = Y[,2], ylim = c(-2, 4), xlim = c(-2, 4),
     ylab = "", xlab ="", bty=  "n", xaxt = "n", yaxt ="n", asp = 1)

points(x = cos(theta), y = sin(theta))
points(0,0, pch = 3)

draw.circle(0, 0, 1, nv = 100, border = NULL, col = NA, lty = 1, lwd = 2)

segments(x0 = cos(theta), x1 = Y[,1], y0 = sin(theta), y1 = Y[,2])
segments(x0 = cos(theta), x1 = 0, y0 = sin(theta), y1 = 0, lty = 2)

dev.off() 

tools::texi2dvi("plotprojecting.tex",pdf=TRUE)


```

\begin{figure}
        \centering

\includegraphics[width=\textwidth]{plotprojecting.pdf}
       
        \caption{A set of datapoints in bivariate space $\boldsymbol{y}_i$ projected onto the circle to produce         a set of datapoints $\boldsymbol{u}_i$. The lines connecting the datapoints to the origin of the circle         represent $r_i$.}
        
		\label{projectingplot}
		
\end{figure}

The projected normal regression model is introduced by @presnell1998projected. 
This model makes use of the embedding approach to circular data. The idea behind
the embedding approach is to not conduct inference on the circular variable 
$\theta$ directly but to indirectly conduct inference on the underlying 
bivariate normal data in $\boldsymbol{y}$. This makes the embedding approach a 
flexible approach to circular data as many different and complex types of models
already exist for bivariate normal data. However, both the vector 
$\boldsymbol{y}$ and value $r$ are quantities that can not be obtained directly 
from $\theta$. The estimation of these objects is a missing data problem. The 
solution for this missing data problem used in this paper relies on Bayesian
estimation. More details on this Bayesian method are outlined by
@nunez2011bayesian and @CremersMulderKlugkist2017 for regression models.

Assuming that $\boldsymbol{y}$ has a bivariate normal distribution, the relation
in Equation \ref{embedding} implies that $\theta$ has a projected normal
distribution. The projected normal (PN) distribution for the cicular outcome
$\theta$ is expressed as:

\begin{equation}\label{PN} PN(\theta \mid \boldsymbol{\mu}, \boldsymbol{I})  =
\frac{1}{2 \pi} e^{-\frac{1}{2}\vert \vert \boldsymbol\mu \vert \vert ^ 2}
\left[1+\frac{\boldsymbol{u}^t\boldsymbol\mu\Phi(\boldsymbol{u}^t\boldsymbol\mu)}{\phi(\boldsymbol{u}^t\boldsymbol\mu)}\right],
\end{equation}

where $\theta$ is measured in radians $-\pi \leq \theta < \pi$,
$\boldsymbol{\mu} = (\mu_{1}, \mu_{2})^{t} \in \mathbb{R}^2$ is the mean vector
of the distribution, the variance-covariance matrix $\boldsymbol{I}$ is an
identity matrix, and $\boldsymbol{u}^{t} = (\cos \theta, \sin \theta)$.  The
terms $\Phi(\cdot)$ and $\phi(\cdot)$ denote the cumulative distribution
function and the probability density function of the standard normal
distribution respectively. When fitting a model using the PN distribution we are
interested in modelling its mean vector $\boldsymbol{\mu}$.

In regression models, $\boldsymbol{\mu}$ has a regression structure. In a
multiple regression model this is specified as follows:

\begin{equation}\label{regression}
\boldsymbol{\mu}_{i} = \begin{pmatrix}
  \mu_{i}^{I}  \vspace{0.2cm}  \\
\mu_{i}^{II}
 \end{pmatrix}=\begin{pmatrix}
  (\boldsymbol{\beta}^{I})^{t}\boldsymbol{x}_{i}^{I} \vspace{0.2cm}  \\
  (\boldsymbol{\beta}^{II})^{t}\boldsymbol{x}_{i}^{II} 
 \end{pmatrix},
\end{equation}

where $I$ and $II$, the two components, are the x and y axis of the Cartesian 
plane, $i = 1, \dots, n$, $\boldsymbol{x}_{i}$ is a vector of predictor values 
for individual $i$ and each $\boldsymbol{\beta}$ is a vector with intercept and 
regression coefficients. To be able to estimate an intercept, the first 
component of $\boldsymbol{x}_{i}$ equals 1. Note that the vectors 
$\boldsymbol{x}_{i}$ are allowed to differ for the two components $I$ and $II$. 
In the next section we will fit this type of model to the motor resonance data.

In terms of interpretation of the circular effect of a variable the two 
component structure in (\ref{regression}) poses a problem. The two components do
not necessarily have a useful interpretation for each type of circular data,
e.g. we cannot talk of a 12 o'clock and 6 o'clock axis in Figure \ref{plothour}.
@CremersMulderKlugkist2017 outline how this problem can be overcome and 
introduce new interpretation tools in PN multiple regression models that 
transform the effects on the two components to an effect on the circle. In this
tutorial we will use these tools and interpret coefficients of a circular effect
and not the coefficients for each of the components.


\subsection{Fitting an ANOVA model to the motor resonance data}\label{RegModelMotor}

In this section we will fit a circular ANOVA model to the motor resonance data 
using the package \verb|bpnreg|. Note that the model from this package is in
fact a regression model that we can speficy in such a way that it is
mathematically equivalent to an ANOVA. First we will give the details on this
model and subsequently we will discuss and interpret the results.


\subsubsection{Fitting the model}\label{fitMotor}

To investigate the effect of condition on the phase difference we specify the 
prediction equation for the mean vector in the projected normal regression model
as follows:

\begin{equation}\label{regressionmotor}
\boldsymbol{\mu}_{i} = \begin{pmatrix}
  \mu_{i}^{I}  \vspace{0.2cm}  \\
\mu_{i}^{II}
 \end{pmatrix}=\begin{pmatrix}
  \beta_{0}^{I} +  \beta_{1}^{I}\text{semi.implicit}_{i}+  \beta_{2}^{I}\text{implicit}_{i} \vspace{0.2cm}  \\
  \beta_{0}^{II} +  \beta_{1}^{II}\text{semi.implicit}_{i}+  \beta_{2}^{II}\text{implicit}_{i}  
 \end{pmatrix},
\end{equation}

where the variables semi.implicit and implicit are dummy variables indicating 
condition membership, $\beta_{0}^{I}$ and $\beta_{0}^{II}$ are the intercepts 
and $\beta_{1}^{I}$, $\beta_{2}^{I}$, $\beta_{1}^{II}$ and $\beta_{2}^{II}$ are 
the regression coefficients of the model. Note that in this model we take the 
explicit observation condition as the reference condition. When we translate
this to the ANOVA context, the intercepts, $\beta_{0}^{I}$ and $\beta_{0}^{II}$,
represent the mean for the explicit condition, $\beta_{0}^{I} + \beta_{1}^{I}$
and $\beta_{0}^{II} + \beta_{1}^{II}$ are expressions for the mean of the
semi-implicit condition and $\beta_{0}^{I} + \beta_{2}^{I}$ and $\beta_{0}^{II}
+ \beta_{2}^{II}$ are expressions for the mean of the implicit condition.

We use the package \verb|bpnreg| to fit the model. Because this is a Bayesian 
method we have to specify some parameters for the MCMC sampler that estimates 
the parameters of the model. MCMC methods are iterative and we thus need to
specify the number of iterations that we want to run. We choose a relatively
high number of the output iterations (\verb|its = 10000|) to make sure that the
sampler converges and that we don't need to run it again in case it did not. We
choose the burn-in period to consist of 100 iterations (\verb|burn = 100|). This
means that we throw away the first 100 iterations to make sure that the
iterations we keep are those at which the sampler has reached its equilibrium.
We also choose the lag, that is how many iterations we want to keep, in this
case every third iteration (\verb|n.lag = 3|). We set a lag to prevent possible
auto-correlation between the parameter estimates. In the next section we will
elaborate further on how to check convergence and choose these MCMC parameters
wisely. But first, we fit the model:

```{r RegModel, cache = TRUE, results = FALSE}
fit.Motor <- bpnr(pred.I = Phaserad ~ 1 + Cond, data = Motor,
                  its = 10000, burn = 100, n.lag = 3, seed = 101)
```


\subsubsection{Convergence}

In a Bayesian model that uses MCMC sampling for estimation we always have to 
assess convergence of the MCMC chain for all parameters in the model. A
traceplot is one way to assess the convergence of a parameter. As an
illustration we will show the traceplot for the MCMC chains for one of the
parameters of the model.

```{r, fig.width=12, fig.height=4, echo = FALSE}
traceplot(fit.Motor, parameter = "circ.diff", variable = "Condimp")
```

From the previous traceplots we may conclude that the MCMC chain converged 
within 10000 iterations and a burn-in of 100. The plot looks like a 
'fat-caterpillar' meaning that the chain has reached an equilibrium around a 
particular value. In case an MCMC chain does not converge we could have add more
iterations, a larger \verb|n.lag| or more burn-in iterations. We can also
evaluate other convergence diagnostics. The focus of this paper however does not
lie on Bayesian data analysis and therefore we refer to other works, e.g. @BDA,
for more information on assessing convergence.


\subsubsection{Results}\label{ResMR}

To answer the question whether the conditions of the motor resonance data differ
in their phase difference we investigate the estimated circular regression 
coefficients. In @CremersMulderKlugkist2017 it is explained how to obtain 
circular regression coefficients for continuous variables and how to interpret 
them. However, because there are no continuous variables in the model we only 
get estimates for the circular means of the three conditions in the data. 
Because we use a Bayesian method we in fact get the posterior distributions of 
these means. Philosophically, in Bayesian statistics each parameter is said to 
have its own distribution. The posterior distribution is the result of the prior
knowledge we have about a parameter before conducting a study, formalized as a 
'prior' distribution\footnote{In this paper we choose non-informative priors for
the parameters.} and the information that lies in the data obtained from a 
study, formalized as the likelihood. The fact that we obtain the distribution of
a parameter is convenient for inference purposes since this means that we do not
just have a point estimate of a parameter (the mean or mode of the posterior
distribution) but we also automatically get an uncertainty estimates (the
standard deviation of the posterior distribution).\footnote{For more background
on Bayesian statistics see e.g. Gelman et.al. (2014).}

\begin{table}
\centering
\caption{Descriptives of the posterior distributions of the circular means of the phase diferencefor the three conditions of the motor resonance data.} 
\begin{tabular}{llllll}
  \hline\noalign{\smallskip}
Condition & Mode & Mean & sd & LB HPD & UB HPD\\ \hline\noalign{\smallskip}
explicit & 42.70$^\circ$ & 45.56$^\circ$ & 11.67$^\circ$ & 22.26$^\circ$ & 67.99$^\circ$\\
semi-implicit & 21.08$^\circ$ & 19.40$^\circ$ & 18.36$^\circ$ & -18.27$^\circ$ & 55.22$^\circ$\\
implicit & 37.22$^\circ$ & 33.47$^\circ$ & 17.77$^\circ$ & -2.25$^\circ$ & 68.22$^\circ$\\
   \hline
\end{tabular}
\label{TableresMRmean}
\end{table}

Summary statistics for the posterior distributions of the circular means  for
each condition are shown in Table \ref{TableresMRmean}. This table shows the
posterior mean, mode, standard deviation (sd) and the lower and upper bound of
the 95$\%$ highest posterior density interval (HPD). The standard deviation of a
posterior is an estimate for the standard error of the parameter. The HPD
interval is the smallest interval in which 95$\%$ of the posterior mass is
located. In terms of interpretation, it is different from a frequentist 
confidence interval since HPD intervals allow for probability statements. For 
example, if the 95$\%$ HPD interval for a parameter $\mu$ runs from 2 to 4 we 
can say that the probability that $\mu$ lies between 2 and 4 is 0.95.

HPD intervals can also be used to test whether a parameter is different from a 
certain value or whether two parameter estimates are different. In Table 
\ref{TableresMRmean} we see that the HPD intervals of the circular means for the
three conditions in the motor resonance data overlap. The cicular mean of the
phase difference is estimated at 47.70$^\circ$ (22.26$^\circ$; 67.99$^\circ$)
for the explicit condition, 37.22$^\circ$ (-2.25$^\circ$; 68.22$^\circ$) for the
implicit condition and 21.08$^\circ$ (-18.27$^\circ$; 55.22$^\circ$) for the
semi-implicit condition. We thus conclude that the circular means for the three
conditions do not differ and that there is no effect of condition on the average
phase difference. \textcolor{red}{Ook de varianties van de drie groepen
vergelijken?}


\subsection{Other approaches to circular ANOVA}\label{otheranova}

In the previous section we have tested whether the mean phase differences of the
three conditions differ in the population using a Bayesian projected normal 
regression model. We can also do this using a frequentist ANOVA for circular 
data that tests the hypothesis $H_0: \mu_{explicit} = \mu_{semi-implicit} = 
\mu_{implicit}$. One of such tests is the Watson-Williams test. This test can be
perfomed using the function \verb|watson.williams.test| in the \verb|circular| 
package and is similar to an ANOVA for linear data interpretation-wise. As in the
projected normal regression model we conclude from this test that the average
phase differences of the three conditions are not significantly different: F(2,
39) = 1.02, p > 0.05.

As in ANOVA models for linear data, we have to meet a set of 
assumptions for this test to be valid. Firstly, the samples from the different 
conditions are assumed to be von-Mises distributed in the Watson-Williams test.
Secondly, the samples are assumed to have the same circular variance. The
von-Mises distribution is a type of distribution for circular data, it is
unimodal with mean $\mu$ and concentration $\kappa$. The assumption of
homogeneity of variance is tested within the \verb|watson.williams.test|
function itself. The assumption of von-Misesness can be tested using e.g. the
Watson's goodness of fit test for the von Mises distribution. If we perform this
test on the phase differences of the three subgroups we conclude that only the
phase differences from the \verb|semi.implicit| and the \verb|implicit|
condition are von-Mises distributed ($H_0$ is not rejected). This means that it
is not completely valid to perform the Watson-Williams test on the motor
resonance data. In addition, the Watson-Williams test does not allow for the
addition of covariates and thus cannot estimate AN(C)OVA models.

```{r, echo = FALSE, results = FALSE}
watson.williams.test(Phaserad ~ Cond, data = Motor)

watson.test(subset(Motor, Cond == "exp")$Phaserad, dist = "vonmises")
watson.test(subset(Motor, Cond == "semi.imp")$Phaserad, dist = "vonmises")
watson.test(subset(Motor, Cond == "imp")$Phaserad, dist = "vonmises")
```

Another method that we can use is a Bayesian circular GLM
that falls within the intrinsic approach. The intrinsic approach to circular 
data differs from the embedding approach in the sense that it uses distributions
that are directly defined on the circle instead of projecting distributions in 
bivariate linear space onto the circle. The Watson-Williams test is thus also a 
method that falls within the intrinsic approach. The Bayesian circular GLM can 
be fit using the package \verb|circglmbayes|. Note that because it is a GLM this
method can also fit ANCOVA and regression models. \textcolor{red}{Dit model ook fitten?}


\section{Mixed-Effects models with a circular outcome}\label{MEModel}

In this section we will introduce the circular mixed-effects model. We will
first introduce a new dataset, the cognitive maps data, and give descriptive
statistics. Then, we will shortly outline the theoretical background to the
mixed-effects model and fit it to the cognitive maps data.


\subsection{The cognitive maps data}\label{CogMap}

The cognitive maps data is a subset of data from a study by @warren2017wormholes
on the geometry of humans' knowledge of navigation space. In their study
@warren2017wormholes amongst others conduct an experiment in which a total of 20
participants used virtual reality headsets to navigate through one of two a 
virtual mazes. The navigation task consisted of walking from a start object to a
target object. In a training phase they had learned to navigate between 
different pairs of start and target objects in one of two versions of the maze.
The number of trials each participant completed in this training phase was
recorded. In the test phase of the experiment participants first walked to a
start object. When they had reached this object the maze dissappeared and only a
"textured groundplane" of the maze remained visible. The participants then
turned toward the location of the target object that they had remembered during
the training phase and started to walk toward the target. The angular difference
between the initial walking direction of a participant from the start object and
the location of the target object, that is, the angular error, was recorded as
an outcome variable in the experiment.

The type of maze is a between-subjects factor, participants either had to
navigate through a 'Euclidean' maze or a 'non-Euclidean' maze. The Euclidean
maze is the standard maze and is a maze just as we know it in the real world.
The other version of the maze, the non-Euclidean maze, has exactly the same
layout as the standard maze but it has virtual features that do not exist
in reality. It namely contains wormholes by which participants can be
'teleported' from one place in the maze to another.

In the test phase of the experiment all participants had to complete 8 trials. 
In each of these trials participants had to walk to a specific target object. A 
within-subjects factor is the type of target object. Pairs of start and target 
objects were of two types: probe and standard. The probe objects were located 
near the entrance and exit of a wormhole in the non-Euclidean maze whereas the
standard objects were located at some distance from the wormholes. For each of
these two types of objects participants had to find 4 different targets
resulting in a total of 8 trials per participant.

For this experiment we could be interested in the question whether the 
participants in the non-Euclidean maze make use of the wormholes when navigating
to the target objects and whether this is true for both the probe and standard
target objects. Due to the design of the mazes the expected angular error was
larger if a participant in the non-Euclidean maze used the wormhole to walk to
the target object. We can thus use the angular error, our outcome variable, to 
differentiate between participants that used the wormhole and those that took 
another path to the target object. Additionally we can control for the amount of
trials that a participant completed in the training phase.


\subsection{Descriptive Statistics}\label{DescriptiveMaps}

The cognitive maps data is incorporated in the package \verb|bpnreg| as the 
dataframe \verb|Maps|. This dataframe has 160 rows; there are 20 subjects that 
each completed 8 trials. The data contains an index variable for the subject 
\verb|Subject| (N = 20) and trial number \verb|Trial.no| (n = 8). It also 
includes variables indicating the type of maze \verb|Maze|, a between-subjects 
factor, and type of trial \verb|Trial.type|, a within-subjects factor. The 
variable \verb|Learn| indicates the amount of learning trials completed. 
\verb|L.c| is a centered version of this variable. The angular error is 
contained in the variables \verb|Error| and \verb|Error.rad| in degrees and 
radians (1 rad = $180/\pi$ degrees) respectively. Descriptives for this data are
shown in Table \ref{TableDescriptivesMaps}. Note that we averaged over subjects
and the trials of each type. The circular mean of the angular error for the
standard trials in the Euclidean maze is thus an average over 10 participants
and 4 trials. We see what the angular errors of both trial types for the
non-Euclidean maze deviate more from 0$^{\circ}$ (direction of the target
object) than for the Euclidean maze.

```{r, echo = FALSE, eval = FALSE}
euclidean <- subset(Maps, Maze == 0)$Error
non.euclidean <- subset(Maps, Maze == 1)$Error

standard.euc <- subset(Maps, Maze == 0 & Trial.type == 0)$Error
standard.neuc <- subset(Maps, Maze == 1 & Trial.type == 0)$Error
probe.euc <- subset(Maps, Maze == 0 & Trial.type == 1)$Error
probe.neuc <- subset(Maps, Maze == 1 & Trial.type == 1)$Error

mean_circ(euclidean, units = "degrees")
mean_circ(non.euclidean, units = "degrees")
mean_circ(standard.euc, units = "degrees")
mean_circ(standard.neuc, units = "degrees")
mean_circ(probe.euc, units = "degrees")
mean_circ(probe.neuc, units = "degrees")

rho_circ(euclidean, units = "degrees")
rho_circ(non.euclidean, units = "degrees")
rho_circ(standard.euc, units = "degrees")
rho_circ(standard.neuc, units = "degrees")
rho_circ(probe.euc, units = "degrees")
rho_circ(probe.neuc, units = "degrees")

eucl <- subset(Maps, Maze == 0)$Learn
neucl <- subset(Maps, Maze == 1 )$Learn

mean(eucl)
mean(neucl)
sd(eucl)
sd(neucl)

```

\begin{table}
\centering
\caption{Descriptives for the cognitive maps data with mean direction ($\bar{\theta}$) and mean resultant length ($\bar{R}$) of the angular error for each condition.} 
\begin{tabular}{lllrr}
  \hline\noalign{\smallskip}
& Maze & Trial.type & $\bar{\theta}$ & $\bar{R}$  \\ \hline\noalign{\smallskip}
Angular error	& Euclidean     & standard & -4.91$^{\circ}$ & 0.89  \\
              &    			      & probe    &  4.46$^{\circ}$ & 0.92   \\
            	& non-Euclidean & standard & -17.59$^{\circ}$ & 0.78  \\
              &    			      & probe    &  37.34$^{\circ}$ & 0.93  \\
   \hline
\end{tabular}
\label{TableDescriptivesMaps}
\end{table}

\subsection{Fitting a mixed-effects model to the cognitive maps data}\label{MEModelMaps}

In this section we will first introduce a circular mixed effects model and fit
this model to the cognitive maps data. Next we discuss the output produced by
the \verb|bpnreg| package. We will discuss the interpretation of fixed and
random effects and model fit.

\subsubsection{The embedding approach for mixed-effects models}\label{embeddingME}

The circular mixed-effects model from the package \verb|bpnreg| is also based on
the embedding approach to circular data. The basic idea behind this approach is
the same as outlined in Section \ref{EmbeddingApproach}. In a real dataset we
have a set of outcome vectors $\boldsymbol{u}_{ij}$, one for each measurement
$j$ within a higher level observation $i$. The solution for the missing data
problem in the estimation of $\boldsymbol{y}_{ij}$ and $r_{ij}$ is outlined in
@nunez2014bayesian.

For the cognitive maps data with $i = 1, \dots, 20$ individuals and $j = 1,
\dots, 8$ measurements per individual we fit a mixed-effects model to
investigate the influence of the type of Maze, type of trial and amount of
learning trials on the angular error. The prediction for the mean vector in this
model, $\boldsymbol{\mu}$, is specified as follows:

\begin{equation}\label{regressionmaps}
\boldsymbol{\mu}_{ij} = \begin{pmatrix}
  \mu_{ij}^{I}  \vspace{0.2cm}  \\
\mu_{ij}^{II}
 \end{pmatrix}=\begin{pmatrix}
  \beta_{0}^{I} +  \beta_{1}^{I}\text{Maze}_{i}+  \beta_{2}^{I}\text{Trial.type}_{ij} + \beta_{3}^{I}\text{L.c}_{i} + b_{0i}^{I} \vspace{0.2cm}  \\
  \beta_{0}^{II} +  \beta_{1}^{II}\text{Maze}_{i}+  \beta_{2}^{II}\text{Trial.type}_{ij} + \beta_{3}^{II}\text{L.c}_{i} + b_{0i}^{II}
 \end{pmatrix},
\end{equation}

where the variables \verb|Maze| and \verb|Trial.type| are dummy variables,
$\beta_{0}^{I}$ and $\beta_{0}^{II}$ are the fixed intercepts, $b_{0i}^{I}$ and
$b_{0i}^{II}$ are the random intercepts and $\beta_{1}^{I}$, $\beta_{2}^{I}$,
$\beta_{3}^{I}$, $\beta_{1}^{II}$, $\beta_{2}^{II}$ and $\beta_{3}^{II}$ are the
fixed regression coefficients of the model. Note that in this model we take the
Euclidean maze and standard trials as reference conditions.

The interpretation problems caused by the two component structure in 
(\ref{regressionmaps}) is of a similar nature as the one in the GLM
model. @longitudinalpaper introduce new tools that solve the interpretation of
circular effects in PN mixed-effects models. In this tutorial we will also use
these tools.

\subsubsection{Fitting the model}\label{fitMaps}

To fit the model in (\ref{regressionmaps}) we use the \verb|bpnme()| function 
from the package \verb|bpnreg|. We also need to specify some parameters for the 
MCMC sampler that estimates the model. We specify the output iterations (10000),
the amount of burn-in (100) and how many iterations we want to keep 
(\verb|n.lag = 3|). Convergence was checked in the same manner as for the
regression model in the previous section and was reached using the settings for
the MCMC algorithm we just specified.

```{r MEModel, cache = TRUE, results = FALSE}
fit.Maps <- bpnme(pred.I = Error.rad ~ Maze + Trial.type + L.c + (1|Subject),
                  data = Maps,
                  its = 10000, burn = 1000, n.lag = 3, seed = 101)
```

Note that the syntax for the model specification in this function is similar to
that of the package \verb|lme4| for fitting (non-circular) mixed-effects models.


\subsubsection{Fixed Effects}\label{fixme}

Next we investigate the coefficients of the fixed effects for this model. First
we show results for the categorical variables type of maze (\verb|Maze|) and
type of trial (\verb|Trial.type|). 

\textbf{Categorical variables}

\begin{table}
\centering
\caption{Descriptives of the posterior of the circular mean of the angular error for each condition.} 
\begin{tabular}{lllrrrrr}
  \hline\noalign{\smallskip}
& Maze & Trial.type & mode & mean & sd & LB HPD & UB HPD  \\ \hline\noalign{\smallskip}
Angular error	& Euclidean     & standard & -12.97$^{\circ}$ & -13.48$^{\circ}$ & 3.9$^{\circ}$ & -21.42$^{\circ}$ & -6.06$^{\circ}$\\
              &    			      & probe    &  11.38$^{\circ}$ & 11.78$^{\circ}$ & 3.29$^{\circ}$ & 5.26$^{\circ}$ & 18.30$^{\circ}$   \\
            	& non-Euclidean & standard & -1.42$^{\circ}$  & -2.04$^{\circ}$  & 6.68$^{\circ}$ & -15.75$^{\circ}$ & 10.49$^{\circ}$ \\
              &    			      & probe    &  31.04$^{\circ}$ & 30.37$^{\circ}$ & 4.31$^{\circ}$ & 22.03$^{\circ}$ & 38.92$^{\circ}$ \\
   \hline
\end{tabular}
\label{TableResMaps}
\end{table}

Table \ref{TableResMaps} shows summary
statistics of the posterior of the average angular error for each of the
categories. Note that because there is a continuous predictor in the model the
posterior estimates represent a marginal effect, they are the effect for an
individual with a 0 score on the continuous predictor \verb|L.c|. Because we
centered this predictor this means that they are the effect for an individual
that has completed an average number of training trials.

By looking at the 95$\%$ HPD intervals of the angular errors in Table 
\ref{TableResMaps} we can test whether the type of maze and type of trial on 
average has an influence on the angular error and thus whether participants make
use of the wormhole. For the standard trials we see that the HPD intervals of
the angular error in the Euclidean and non-Euclidean overlaps and that thus the 
angular error is not different. This means that in the standard trials the 
participants on average did not make use of the wormholes in the non-Euclidean 
maze. For the probe trials however, the HPD intervals of the Euclidean and 
non-Euclidean does overlap and thus the angular error is different. This means 
that in the probe trials, the participants on average did make use of the 
wormholes in the non-Euclidean maze.


\textbf{Continuous variables}

For the continuous variable \verb|L.c| we get a set of parameters, $b_c$, $SAM$
and $AS$, describing its effect on the circle. How these parameters are computed
is described in @CremersMulderKlugkist2017 and @longitudinalpaper. In this paper 
we will only focus on how to interpret them. 

```{r circregline, cache = TRUE, warning = FALSE, dev = "tikz",fig.ext="svg", echo = FALSE, results = FALSE}
set.seed(101)

Xreal <- rnorm(100, -2, 1.5)

CompI  <- 1 + 0.3*Xreal + rnorm(100, 0.1,1)
CompII <- 2 + -0.4*Xreal + rnorm(100, 0.1,1)

Outreal <- atan2(CompII, CompI)

X <- seq(-5, 5, 0.1)

predCompI  <- 1.1 + 0.25*X
predCompII <- 2.1 + -0.4*X

PredCirc <- atan2(predCompII, predCompI)

axval <- ((1.1*0.25)+(2.1*-0.4))/(1.1^2 + 2.1^2)
acval <- atan2(2.1 + -0.4*axval, 1.1 + 0.25*axval)

tikz("circregline.tex", standAlone =TRUE, height = 3.5, width = 6, pointsize = 12, engine = "pdftex")

plot(X, PredCirc*(180/pi), ylim = c(-200,200), type = "l", xlab = "x",
     ylab = "$\\hat{\\theta}$", bty = 'n', yaxt = "n", xaxt = "n")
axis(1, xaxp = c(-5, 0, 5), at = c(seq(-5, 5, by = 1)),
     labels = c(seq(-5, 5, by = 1)), las = 2)
axis(2, xaxp = c(-180, 0,180), at = c(seq(-180, 180, by = 60)),
     labels = c(seq(-180, 180, by = 60)), las = 2)
points(Xreal, Outreal*(180/pi), cex=0.5)
points(axval,
       acval*(180/pi),
       pch = 0)

dev.off()

tools::texi2dvi("circregline.tex",pdf=TRUE)


```

\begin{figure}
\includegraphics{circregline.pdf}
\caption{Predicted circular regression line for the relation between a linear predictor $x$ and a predicted circular outcome $\theta$ together with the original datapoints. The square indicates the inflection point of the regression line.} 
\label{circregline}
\end{figure}

In Figure \ref{circregline} a circular regression line for the effect of a
predictor $x$ on the circular outcome is shown. This regression line can be
described using the three circular coefficients $b_c$, $SAM$ and $AS$. The
coefficient $b_c$ represents the slope of the circular regression line at the
inflection point (the square in Figure \ref{circregline}). However, this may not
be a representative effect for each dataset as the inflection point can lie in
the extremes of the data (as in Figure \ref{circregline}) or even completely
outside the range of the predictor $x$. Therefore two additional circular
coefficients were developed by @CremersMulderKlugkist2017, $SAM$ and $AS$. The
coefficient $SAM$ represents the slope of the circular regression line at the
average of the predictor $\bar{x}$ and the coefficients $AS$ represents the
average slope over all of the predictor values.

\begin{table}
\centering
\caption{Descriptives of the posterior of the coefficients of the effect of L.c on the angular error.} 
\begin{tabular}{lrrrrr}
  \hline\noalign{\smallskip}
Coefficient & mode & mean & sd & LB HPD & UB HPD  \\ \hline\noalign{\smallskip}
$b_c$ & -0.89$^{\circ}$ & -0.21$^{\circ}$ & 1.73$^{\circ}$ & -2.84$^{\circ}$ & 2.55$^{\circ}$\\
$SAM$ &  0.58$^{\circ}$ & -0.84$^{\circ}$ & 90.80$^{\circ}$ & -11.51$^{\circ}$ & 12.73$^{\circ}$   \\
$AS$  & -0.63$^{\circ}$  & -1.11$^{\circ}$  & 92.44$^{\circ}$ & -13.17$^{\circ}$ & 13.14$^{\circ}$ \\
   \hline
\end{tabular}
\label{TableResMapsLc}
\end{table}

For the effect of L.c on the angular error in the cognitive maps data, the HPD 
intervals for all three circular coefficients, $b_c$, $SAM$ and $AS$ include 0
(see Table \ref{TableResMapsLc}. Thus we conclude that at the inflection point,
at the average predictor value and on average \verb|L.c| the number of training
trials does not influence the angular error. This is in fact a good thing, since
it shows that the training phase of the experiment worked to get all
participants at the same level. For educational purposes we will however
continue to give the interpretation of the coefficients. The $SAM$ is
interpreted as follows: at the average \verb|L.c|, for a 1 unit increase in
\verb|L.c| the angular error increases with 0.58 degrees. The $AS$ can be
interpreted as: on average, for a 1 unit increase in \verb|L.c| the angular
error decreases with 0.63 degrees. The $b_c$ can be interpreted as: at the
inflection point, for a 1 unit increase in \verb|L.c| the angular error
decreases with 0.89 degrees.


\subsubsection{Random Effects}\label{ranme}

In mixed-effects models we are also interested in evaluating the variance of the
random effects. In the model for the cognitive maps data we included a random
intercept. This means that we estimate a separate intercept for each
participant. How to compute random effect variances on the circle is outlined in
@longitudinalpaper. For the cognitive maps data the posterior mode of the
intercept variance on the circle is estimated at $3.5*10^{-5}$ and its HPD
interval is $(4.2*10^{-6}; 1.4*10^{-3})$. This variance is very low which means
that the participants do not differ a lot in their individual intercept
estimates. Note that this is not necessarily problematic. In some cases we are
not interested in the variances of the random efects but simply want to fit a
mixed-effects model because we have within factors, such as \verb|Trial.type|,
that cannot be properly incorporated in a standard regression model.


\subsubsection{Model Comparison}\label{fitme}

When fitting mixed-effects (or multilevel) models we often fit a set of nested
models to our data and follow a model building strategy [@hox2002multilevel].
This can be done top-down, starting with the most complex model, or bottom-up,
starting with the simplest model. Here we use a bottom-up strategy and start
with the so called intercept-only model, a model containing only a fixed and
random intercept:

```{r MEModelIO, cache = TRUE, results = FALSE}

fit.MapsIO <- bpnme(pred.I = Error.rad ~ (1|Subject),
                    data = Maps,
                    its = 10000, burn = 1000, n.lag = 3, seed = 101)
```

We then update this model with fixed effects for the predictors at the lowest
level (within-subjects factors), in this case only one, \verb|Trial.type|. We do
this to check whether the set of predictors improved the fit of the model and
can explain a part of the random intercept variance from the intercept-only
model.

```{r MEModel1p, cache = TRUE, results = FALSE}
fit.Maps1p <- bpnme(pred.I = Error.rad ~ Trial.type + (1|Subject),
                    data = Maps,
                    its = 10000, burn = 1000, n.lag = 3, seed = 101)
```

We then add fixed effects for the predictors at the higher level
(between-subjects factors), in this case \verb|Maze| and \verb|L.c|. Again we do
this to check whether they improve the fit of the model and whether they can
explain a part of the random intercept variance.

```{r, eval = FALSE}
fit.Maps <- bpnme(pred.I = Error.rad ~ Maze + Trial.type + L.c + (1|Subject),
                  data = Maps,
                  its = 10000, burn = 1000, n.lag = 3, seed = 101)
```


Because we have already seen that the effect of \verb|L.c| was not different
from 0 we also fit the model with only the \verb|Maze| and \verb|Trial.type|
predictors.

```{r MEModel2, cache = TRUE, results = FALSE}
fit.Maps2 <- bpnme(pred.I = Error.rad ~ Maze + Trial.type + (1|Subject),
                  data = Maps,
                  its = 10000, burn = 1000, n.lag = 3, seed = 101)
```

Additional steps, such as adding random slopes for first level predictors and
cross-level interactions, can be taken. In this paper we will however stick to
the previous three models.

\textbf{Model fit}

To assess the fit of the models we look at 4 different model fit criteria: two
version of the deviance information criterion (DIC and DIC$_{alt}$) and two
versions of the Watanabe-Akaike information criterion (WAIC$_1$ and WAIC$_2$)
@BDA. We choose these four criteria because they are specifically useful in
Bayesian models where MCMC methods have been used to estimate the parameters.
All four criteria have a fit part consisting of a measure based on the
loglikelihood and include a penalty in the form of an effective number of
parameters. For all criteria lower values indicate better fit. @BDA describes
how to compute these criteria. Table \ref{TableModelFitMaps} shows these
criteria for 4 different models.


\begin{table}
\centering
\caption{Model fit criteria for several models fit to the cognitive maps data.} 
\begin{tabular}{lrrrr}
  \hline\noalign{\smallskip}
 Criterion & Intercept-only & \verb|Trial.type| & \verb|Trial.type| + \verb|Maze| & \verb|Trial.type| + \verb|Maze| + \verb|L.c|  \\ \hline\noalign{\smallskip}
DIC	            & 304.61 & 267.91 & 253.97 & 257.94 \\
DIC$_{alt}$     & 324.33 & 286.97 & 257.14 & 260.78 \\
WAIC$_1$        & 308.41 & 271.61 & 255.00 & 258.41 \\
WAIC$_2$        & 308.43 & 271.77 & 255.40 & 259.02 \\
   \hline
\end{tabular}
\label{TableModelFitMaps}
\end{table}

In the results for the example we see that the fit improves in all 4 model
diagnostics for each model except for the last one. This means that the
predictor \verb|Trial.type| improves the fit of the model over the
intercept-only model and that the predictors \verb|Maze| and \verb|Trial.type|
together improve the fit of the model over the model with only the
\verb|Trial.type| predictor. Because the variable \verb|L.c| had no effect it is
logical that this predictor does not improve the fit of the most right model
over the model with the \verb|Maze| and \verb|Trial.type| predictors. We
conclude that the model with the predictors \verb|Trial.type| and \verb|Maze|
fits best.

\textbf{Explained variance}

Apart from information about whether adding predictors improves the fit of the
model we are also interested in whether these predictors explain a part of the
random effect variances. For the cognitive maps data we are interested in
whether the \verb|Maze| and \verb|Trial.type| predictors explain a part of the
variance in individual intercepts. To assess this we compare the posterior
estimates of the circular random intercept for the intercept-only model and the
model with the \verb|Maze| and \verb|Trial.type| predictors.

The posterior mode of the intercept variance in the intercept-only model equals
$6.61*10^{-5} (8.20*10^{-6}; 3.62*10^{-3})$, the posterior mode of the intercept
variance in the model with the \verb|Maze| and \verb|Trial.type| predictors
equals $3.25*10^{-5} (4.40*10^{-6}; 1.59*10^{-3})$. First of all, note that in
the intercept-only model there is almost to random intercept variance. The
posterior mode of the circular variance is very close to 0. Compared to the
estimates of the variance in the intercept-only model there is hardly any change
in estimates for the variance in the mode with \verb|Maze| and
\verb|Trial.type|.Furthermore, their HPD intervals have a very large overlap. We
thus conclude that the variables \verb|Maze| and \verb|Trial.type| did not
explain any variance in the random intercepts. This makes sense as there was
harly any intercept variance in the intercept-only model to begin with.





\section{Concluding remarks}\label{conclusion}

In this paper we have given a tutorial for researchers in cognitive psychology
on how to analyse circular data using the package \verb|bpnreg|. We have covered
data inspection in Section \ref{DataInspection}, the fitting of a Bayesian
circular GLM in Section \ref{RegModel} and the fitting of a Bayesain
mixed-effects model in Section \ref{MEModel}. We have also given a short
introduction into the theoretical background of these models in Section
\ref{EmbeddingApproach} and \ref{embeddingME}.

Apart from the embedding approach to circular data, as used in this tutorial,
there are two other approaches to the analysis of circular data. In the wrapping
approach the data on the circle is assumed to have origined from wrapping a
univariate distribution on the real line onto the circle. In the intrinsic
approach distributions, such as the von Mises distribution, are directly defined
on the circle. For both approaches models have been described in the literature
[@ravindran2011bayesian;@lagona2016regression;@gill2010;@fisher1992regression;@mulder2017bayesian].
The regression model using the intrinsic approach from @fisher1992regression is
implemented in the package \verb|circular| and the circular general linear model
from @mulder2017bayesian is implemented in the package \verb|circglmmbayes|. For
neither approach however a detailed tutorial describing how to analyze circular
data using the functions from their package has been written thus far.
Furthermore, the PN approach to circular modelling has the additional advantage
that it is relatively easy to fit more complex models, e.g. the mixed-effects
model in this tutorial.

\newpage
# References

